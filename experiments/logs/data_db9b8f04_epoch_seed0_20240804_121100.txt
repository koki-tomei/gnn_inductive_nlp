23d6a65c774d
pid: 40985
screen: 

gpu: 0,1

Namespace(LMentemb=True, LMrelemb=True, att_head_num=2, batch_size=25, classify_relation=18, clutrr=True, compile_mlp_queryrep=False, concept_in_dim=100, concept_num=40, cuda=True, data_artifact_dir='data_db9b8f04_relationEnt_onechoice', data_id='data_db9b8f04', dataset='clutrr', debug=False, decoder_lr=0.001, decoder_model='compile', dev_adj='NoneS', dev_statements='NoneS', drop_partial_batch=False, dropoutf=0.2, dropoutg=0.2, dropouti=0.2, edge_pruning_order='const', edge_pruning_ratio=0.5, edge_scoring=True, edgeent_position='forp', encoder='roberta-large', encoder_layer=-1, encoder_lr=1e-05, ent_emb=['tzw'], ent_emb_paths=['data/cpnet/tzw.ent.npy'], ent_format='relation', eval_batch_size=25, fc_dim=100, fc_layer_num=1, fc_linear_sent=False, fill_partial_batch=False, fp16=True, freeze_ent_emb=True, gnn_dim=100, inhouse=False, inhouse_train_qids='data/clutrr/inhouse_split_qids.txt', init_range=0.02, initemb_method='onehot-LM', k=5, last_unfreeze_layer=1, load_model_path=None, log_interval=50, loss='cross_entropy', lr_schedule='fixed', max_epochs_before_stop=10, max_grad_norm=1.0, max_node_num=13, max_seq_len=-100, mini_batch_size=25, mode='train', model_art_pname='qagnn-entrel', modelsavedir_local='saved_models/tmp_model', n_epochs=200, num_relation=8, one_choice=True, optim='radam', refreeze_epoch=10000, save_dir='saved_models/clutrr/enc-roberta-large__k5__gnndim100__bs25__seed0__20240804_121100', save_model=False, scored_edge_norm='disabled', seed=0, sentence_level=False, simple=False, start_pruning_epoch=40, subsample=1.0, sweep=False, test_adj='NoneS', test_statements='NoneS', testk=-1, train_adj='NoneS', train_statements='data/clutrr/data_db9b8f04_relationEnt_onechoice/after_processTESTdata.pkl', unfreeze_epoch=1000, use_cache=True, valid_set=0.1, wandbmode='disabled', warmup_steps=150, weight_decay=0.01, wnb_project='gnn-inductive')
| num_concepts: 40 |
model_type roberta
train_statement_path data/clutrr/data_db9b8f04_relationEnt_onechoice/after_processTESTdata.pkl
train_sample_num: 13574
dev_sample_num: 1509
clutrr train_max_seqlen: 249
clutrr test_max_seqlens: [249, 249, 249, 249, 249, 249, 267, 307, 327]
num_choice 1
args.num_relation 8
parameters:
 ===decoder===
	fc.layers.0-Linear.weight                    	trainable	torch.Size([100, 1524])	device:cuda:0
	fc.layers.0-Linear.bias                      	trainable	torch.Size([100])	device:cuda:0
	fc.layers.0-LayerNorm.weight                 	trainable	torch.Size([100])	device:cuda:0
	fc.layers.0-LayerNorm.bias                   	trainable	torch.Size([100])	device:cuda:0
	fc.layers.1-Linear.weight                    	trainable	torch.Size([18, 100])	device:cuda:0
	fc.layers.1-Linear.bias                      	trainable	torch.Size([18])	device:cuda:0
	Wi_node_context.weight                       	trainable	torch.Size([100, 1029])	device:cuda:0
	Wi_node_non_context.weight                   	trainable	torch.Size([100, 1029])	device:cuda:0
	Wi_edge_context.weight                       	trainable	torch.Size([100, 2056])	device:cuda:0
	Wi_edge_non_context.weight                   	trainable	torch.Size([100, 2056])	device:cuda:0
	edge_scoring_linear.weight                   	trainable	torch.Size([1, 2048])	device:cuda:0
	edge_scoring_linear.bias                     	trainable	torch.Size([1])	device:cuda:0
	edge_feat2emb.weight                         	trainable	torch.Size([100, 110])	device:cuda:0
	input_attention1.weight                      	trainable	torch.Size([100, 200])	device:cuda:0
	input_attention2.weight                      	trainable	torch.Size([1, 100])	device:cuda:0
	W_h_bond_0.weight                            	trainable	torch.Size([100, 100])	device:cuda:0
	Attention1_0.weight                          	trainable	torch.Size([100, 200])	device:cuda:0
	Attention2_0.weight                          	trainable	torch.Size([1, 100])	device:cuda:0
	W_h_bond_1.weight                            	trainable	torch.Size([100, 100])	device:cuda:0
	Attention1_1.weight                          	trainable	torch.Size([100, 200])	device:cuda:0
	Attention2_1.weight                          	trainable	torch.Size([1, 100])	device:cuda:0
	W_h_bond_2.weight                            	trainable	torch.Size([100, 100])	device:cuda:0
	Attention1_2.weight                          	trainable	torch.Size([100, 200])	device:cuda:0
	Attention2_2.weight                          	trainable	torch.Size([1, 100])	device:cuda:0
	W_h_bond_3.weight                            	trainable	torch.Size([100, 100])	device:cuda:0
	Attention1_3.weight                          	trainable	torch.Size([100, 200])	device:cuda:0
	Attention2_3.weight                          	trainable	torch.Size([1, 100])	device:cuda:0
	W_o.weight                                   	trainable	torch.Size([100, 200])	device:cuda:0
	W_o.bias                                     	trainable	torch.Size([100])	device:cuda:0
	gru.bias                                     	trainable	torch.Size([100])	device:cuda:0
	gru.gru.weight_ih_l0                         	trainable	torch.Size([300, 100])	device:cuda:0
	gru.gru.weight_hh_l0                         	trainable	torch.Size([300, 100])	device:cuda:0
	gru.gru.bias_ih_l0                           	trainable	torch.Size([300])	device:cuda:0
	gru.gru.bias_hh_l0                           	trainable	torch.Size([300])	device:cuda:0
	gru.gru.weight_ih_l0_reverse                 	trainable	torch.Size([300, 100])	device:cuda:0
	gru.gru.weight_hh_l0_reverse                 	trainable	torch.Size([300, 100])	device:cuda:0
	gru.gru.bias_ih_l0_reverse                   	trainable	torch.Size([300])	device:cuda:0
	gru.gru.bias_hh_l0_reverse                   	trainable	torch.Size([300])	device:cuda:0
	communicate_mlp.weight                       	trainable	torch.Size([100, 300])	device:cuda:0
	W_h_atom_0.weight                            	trainable	torch.Size([100, 100])	device:cuda:0
	W_h_atom_1.weight                            	trainable	torch.Size([100, 100])	device:cuda:0
	W_h_atom_2.weight                            	trainable	torch.Size([100, 100])	device:cuda:0
	W_h_atom_3.weight                            	trainable	torch.Size([100, 100])	device:cuda:0
	decoder (unfreeze) total: 1136467

-----------------------------------------------------------------------
Using fp16 training
	module.encoder.layer.23.attention.self.query.weight	trainable	torch.Size([1024, 1024])	device:cuda:0
	module.encoder.layer.23.attention.self.query.bias	trainable	torch.Size([1024])	device:cuda:0
	module.encoder.layer.23.attention.self.key.weight	trainable	torch.Size([1024, 1024])	device:cuda:0
	module.encoder.layer.23.attention.self.key.bias	trainable	torch.Size([1024])	device:cuda:0
	module.encoder.layer.23.attention.self.value.weight	trainable	torch.Size([1024, 1024])	device:cuda:0
	module.encoder.layer.23.attention.self.value.bias	trainable	torch.Size([1024])	device:cuda:0
	module.encoder.layer.23.attention.output.dense.weight	trainable	torch.Size([1024, 1024])	device:cuda:0
	module.encoder.layer.23.attention.output.dense.bias	trainable	torch.Size([1024])	device:cuda:0
	module.encoder.layer.23.attention.output.LayerNorm.weight	trainable	torch.Size([1024])	device:cuda:0
	module.encoder.layer.23.attention.output.LayerNorm.bias	trainable	torch.Size([1024])	device:cuda:0
	module.encoder.layer.23.intermediate.dense.weight	trainable	torch.Size([4096, 1024])	device:cuda:0
	module.encoder.layer.23.intermediate.dense.bias	trainable	torch.Size([4096])	device:cuda:0
	module.encoder.layer.23.output.dense.weight  	trainable	torch.Size([1024, 4096])	device:cuda:0
	module.encoder.layer.23.output.dense.bias    	trainable	torch.Size([1024])	device:cuda:0
	module.encoder.layer.23.output.LayerNorm.weight	trainable	torch.Size([1024])	device:cuda:0
	module.encoder.layer.23.output.LayerNorm.bias	trainable	torch.Size([1024])	device:cuda:0
	encoder unfreeze total: 12596224
	whole encoder total: 355359744
	whole unfreeze total: 13732691
| step    49 |  lr: 0.0000100 | loss  2.8479 | ms/batch  225.25 |
| step    99 |  lr: 0.0000100 | loss  2.7909 | ms/batch  224.85 |
| step   149 |  lr: 0.0000100 | loss  2.7827 | ms/batch  224.88 |
| step   199 |  lr: 0.0000100 | loss  2.7781 | ms/batch  224.41 |
| step   249 |  lr: 0.0000100 | loss  2.7822 | ms/batch  225.14 |
| step   299 |  lr: 0.0000100 | loss  2.7773 | ms/batch  225.40 |
| step   349 |  lr: 0.0000100 | loss  2.7803 | ms/batch  226.43 |
| step   399 |  lr: 0.0000100 | loss  2.7736 | ms/batch  227.77 |
| step   449 |  lr: 0.0000100 | loss  2.7878 | ms/batch  226.73 |
| step   499 |  lr: 0.0000100 | loss  2.7554 | ms/batch  227.81 |
-----------------------------------------------------------------------
| epoch   0 | step   543 | dev_acc  0.1160 |
| test/2_acc  0.4737 |
| test/3_acc  0.1495 |
| test/4_acc  0.0779 |
| test/5_acc  0.0757 |
| test/6_acc  0.0476 |
| test/7_acc  0.0710 |
| test/8_acc  0.0593 |
| test/9_acc  0.0484 |
| test/10_acc  0.0984 |
-----------------------------------------------------------------------
| step   549 |  lr: 0.0000100 | loss  2.7397 | ms/batch  752.06 |
| step   599 |  lr: 0.0000100 | loss  2.7498 | ms/batch  227.98 |
| step   649 |  lr: 0.0000100 | loss  2.7715 | ms/batch  228.24 |
| step   699 |  lr: 0.0000100 | loss  2.7249 | ms/batch  227.36 |
| step   749 |  lr: 0.0000100 | loss  2.6527 | ms/batch  228.40 |
| step   799 |  lr: 0.0000100 | loss  2.6325 | ms/batch  228.70 |
| step   849 |  lr: 0.0000100 | loss  2.6265 | ms/batch  228.34 |
| step   899 |  lr: 0.0000100 | loss  2.5804 | ms/batch  228.26 |
| step   949 |  lr: 0.0000100 | loss  2.5197 | ms/batch  228.21 |
| step   999 |  lr: 0.0000100 | loss  2.5065 | ms/batch  228.11 |
| step  1049 |  lr: 0.0000100 | loss  2.4578 | ms/batch  227.89 |
-----------------------------------------------------------------------
| epoch   1 | step  1086 | dev_acc  0.1995 |
| test/2_acc  0.6579 |
| test/3_acc  0.0561 |
| test/4_acc  0.1299 |
| test/5_acc  0.1027 |
| test/6_acc  0.0381 |
| test/7_acc  0.0581 |
| test/8_acc  0.0667 |
| test/9_acc  0.0645 |
| test/10_acc  0.0656 |
-----------------------------------------------------------------------
| step  1099 |  lr: 0.0000100 | loss  2.4126 | ms/batch  758.43 |
| step  1149 |  lr: 0.0000100 | loss  2.3746 | ms/batch  227.74 |
| step  1199 |  lr: 0.0000100 | loss  2.3540 | ms/batch  227.93 |
| step  1249 |  lr: 0.0000100 | loss  2.3268 | ms/batch  228.64 |
| step  1299 |  lr: 0.0000100 | loss  2.2814 | ms/batch  228.14 |
| step  1349 |  lr: 0.0000100 | loss  2.2588 | ms/batch  228.00 |
| step  1399 |  lr: 0.0000100 | loss  2.2898 | ms/batch  228.39 |
| step  1449 |  lr: 0.0000100 | loss  2.2620 | ms/batch  228.63 |
| step  1499 |  lr: 0.0000100 | loss  2.1847 | ms/batch  227.15 |
| step  1549 |  lr: 0.0000100 | loss  2.1892 | ms/batch  228.29 |
| step  1599 |  lr: 0.0000100 | loss  2.1139 | ms/batch  227.69 |
-----------------------------------------------------------------------
| epoch   2 | step  1629 | dev_acc  0.3141 |
| test/2_acc  0.6579 |
| test/3_acc  0.2336 |
| test/4_acc  0.2078 |
| test/5_acc  0.1459 |
| test/6_acc  0.0857 |
| test/7_acc  0.0968 |
| test/8_acc  0.0667 |
| test/9_acc  0.0645 |
| test/10_acc  0.1148 |
-----------------------------------------------------------------------
| step  1649 |  lr: 0.0000100 | loss  2.1533 | ms/batch  758.68 |
| step  1699 |  lr: 0.0000100 | loss  2.0711 | ms/batch  227.33 |
| step  1749 |  lr: 0.0000100 | loss  2.0728 | ms/batch  228.48 |
| step  1799 |  lr: 0.0000100 | loss  2.1207 | ms/batch  229.19 |
| step  1849 |  lr: 0.0000100 | loss  2.0780 | ms/batch  228.50 |
| step  1899 |  lr: 0.0000100 | loss  2.0253 | ms/batch  227.94 |
